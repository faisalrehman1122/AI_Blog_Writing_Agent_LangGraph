{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Blog Writing Agent - Complete Implementation\n",
        "\n",
        "This notebook implements an AI agent that automatically plans, researches, and writes technical blog posts using LangGraph.\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "**Input:** A topic string (e.g., \"Write a blog on Self Attention\")\n",
        "\n",
        "**Output:** \n",
        "- A complete markdown blog post saved as a `.md` file\n",
        "- Optional images generated and placed in the blog (saved in `images/` directory)\n",
        "- The blog includes proper citations when research is performed\n",
        "\n",
        "## Architecture\n",
        "\n",
        "The agent uses a LangGraph workflow with the following nodes:\n",
        "1. **Router** - Decides if web research is needed\n",
        "2. **Research** - Performs web searches using Tavily API\n",
        "3. **Orchestrator** - Creates a detailed blog plan with sections\n",
        "4. **Workers** - Write individual sections in parallel\n",
        "5. **Reducer** - Merges sections, decides on images, generates images, and saves the final blog\n",
        "\n",
        "## Setup Requirements\n",
        "\n",
        "- OpenAI API key (for GPT models)\n",
        "- Tavily API key (optional, for web research)\n",
        "- Google API key (optional, for image generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies and Setup Environment\n",
        "\n",
        "First, we need to install required packages and load environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import operator\n",
        "import os\n",
        "import re\n",
        "from datetime import date, timedelta\n",
        "from pathlib import Path\n",
        "from typing import TypedDict, List, Optional, Literal, Annotated\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.types import Send\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define Data Schemas\n",
        "\n",
        "We define Pydantic models to structure the data flow through our agent:\n",
        "- **Task**: Represents a single section of the blog with its requirements\n",
        "- **Plan**: The overall blog structure with title, audience, tone, and list of tasks\n",
        "- **EvidenceItem**: Web search results that can be cited\n",
        "- **RouterDecision**: Decision about whether research is needed\n",
        "- **ImageSpec**: Specification for generating images\n",
        "- **State**: The complete state that flows through the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Task(BaseModel):\n",
        "    id: int\n",
        "    title: str\n",
        "    goal: str = Field(..., description=\"One sentence describing what the reader should do/understand.\")\n",
        "    bullets: List[str] = Field(..., min_length=3, max_length=6)\n",
        "    target_words: int = Field(..., description=\"Target words (120‚Äì550).\")\n",
        "    tags: List[str] = Field(default_factory=list)\n",
        "    requires_research: bool = False\n",
        "    requires_citations: bool = False\n",
        "    requires_code: bool = False\n",
        "\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    blog_title: str\n",
        "    audience: str\n",
        "    tone: str\n",
        "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
        "    constraints: List[str] = Field(default_factory=list)\n",
        "    tasks: List[Task]\n",
        "\n",
        "\n",
        "class EvidenceItem(BaseModel):\n",
        "    title: str\n",
        "    url: str\n",
        "    published_at: Optional[str] = None\n",
        "    snippet: Optional[str] = None\n",
        "    source: Optional[str] = None\n",
        "\n",
        "\n",
        "class RouterDecision(BaseModel):\n",
        "    needs_research: bool\n",
        "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
        "    reason: str\n",
        "    queries: List[str] = Field(default_factory=list)\n",
        "    max_results_per_query: int = Field(5)\n",
        "\n",
        "\n",
        "class EvidencePack(BaseModel):\n",
        "    evidence: List[EvidenceItem] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "class ImageSpec(BaseModel):\n",
        "    placeholder: str = Field(..., description=\"e.g. [[IMAGE_1]]\")\n",
        "    filename: str = Field(..., description=\"Save under images/, e.g. qkv_flow.png\")\n",
        "    alt: str\n",
        "    caption: str\n",
        "    prompt: str = Field(..., description=\"Prompt to send to the image model.\")\n",
        "    size: Literal[\"1024x1024\", \"1024x1536\", \"1536x1024\"] = \"1024x1024\"\n",
        "    quality: Literal[\"low\", \"medium\", \"high\"] = \"medium\"\n",
        "\n",
        "\n",
        "class GlobalImagePlan(BaseModel):\n",
        "    md_with_placeholders: str\n",
        "    images: List[ImageSpec] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    topic: str\n",
        "    mode: str\n",
        "    needs_research: bool\n",
        "    queries: List[str]\n",
        "    evidence: List[EvidenceItem]\n",
        "    plan: Optional[Plan]\n",
        "    as_of: str\n",
        "    recency_days: int\n",
        "    sections: Annotated[List[tuple[int, str]], operator.add]\n",
        "    merged_md: str\n",
        "    md_with_placeholders: str\n",
        "    image_specs: List[dict]\n",
        "    final: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Initialize LLM\n",
        "\n",
        "We use OpenAI's GPT-4.1-mini model for all text generation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Router Node\n",
        "\n",
        "The router decides whether web research is needed based on the topic:\n",
        "- **closed_book**: Evergreen topics that don't need recent information\n",
        "- **hybrid**: Topics that need some up-to-date examples but are mostly evergreen\n",
        "- **open_book**: Volatile topics like news, latest releases, pricing that require fresh data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
        "\n",
        "Decide whether web research is needed BEFORE planning.\n",
        "\n",
        "Modes:\n",
        "- closed_book (needs_research=false): evergreen concepts.\n",
        "- hybrid (needs_research=true): evergreen + needs up-to-date examples/tools/models.\n",
        "- open_book (needs_research=true): volatile weekly/news/\"latest\"/pricing/policy.\n",
        "\n",
        "If needs_research=true:\n",
        "- Output 3‚Äì10 high-signal, scoped queries.\n",
        "- For open_book weekly roundup, include queries reflecting last 7 days.\n",
        "\"\"\"\n",
        "\n",
        "def router_node(state: State) -> dict:\n",
        "    decider = llm.with_structured_output(RouterDecision)\n",
        "    decision = decider.invoke(\n",
        "        [\n",
        "            SystemMessage(content=ROUTER_SYSTEM),\n",
        "            HumanMessage(content=f\"Topic: {state['topic']}\\nAs-of date: {state['as_of']}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if decision.mode == \"open_book\":\n",
        "        recency_days = 7\n",
        "    elif decision.mode == \"hybrid\":\n",
        "        recency_days = 45\n",
        "    else:\n",
        "        recency_days = 3650\n",
        "\n",
        "    return {\n",
        "        \"needs_research\": decision.needs_research,\n",
        "        \"mode\": decision.mode,\n",
        "        \"queries\": decision.queries,\n",
        "        \"recency_days\": recency_days,\n",
        "    }\n",
        "\n",
        "def route_next(state: State) -> str:\n",
        "    return \"research\" if state[\"needs_research\"] else \"orchestrator\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Research Node\n",
        "\n",
        "If research is needed, this node uses Tavily API to search the web and collect evidence that can be cited in the blog. It filters results by recency for open_book mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
        "    if not os.getenv(\"TAVILY_API_KEY\"):\n",
        "        return []\n",
        "    try:\n",
        "        from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "        tool = TavilySearchResults(max_results=max_results)\n",
        "        results = tool.invoke({\"query\": query})\n",
        "        out: List[dict] = []\n",
        "        for r in results or []:\n",
        "            out.append(\n",
        "                {\n",
        "                    \"title\": r.get(\"title\") or \"\",\n",
        "                    \"url\": r.get(\"url\") or \"\",\n",
        "                    \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
        "                    \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
        "                    \"source\": r.get(\"source\"),\n",
        "                }\n",
        "            )\n",
        "        return out\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def _iso_to_date(s: Optional[str]) -> Optional[date]:\n",
        "    if not s:\n",
        "        return None\n",
        "    try:\n",
        "        return date.fromisoformat(s[:10])\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer.\n",
        "\n",
        "Given raw web search results, produce EvidenceItem objects.\n",
        "\n",
        "Rules:\n",
        "- Only include items with a non-empty url.\n",
        "- Prefer relevant + authoritative sources.\n",
        "- Normalize published_at to ISO YYYY-MM-DD if reliably inferable; else null (do NOT guess).\n",
        "- Keep snippets short.\n",
        "- Deduplicate by URL.\n",
        "\"\"\"\n",
        "\n",
        "def research_node(state: State) -> dict:\n",
        "    queries = (state.get(\"queries\") or [])[:10]\n",
        "    raw: List[dict] = []\n",
        "    for q in queries:\n",
        "        raw.extend(_tavily_search(q, max_results=6))\n",
        "\n",
        "    if not raw:\n",
        "        return {\"evidence\": []}\n",
        "\n",
        "    extractor = llm.with_structured_output(EvidencePack)\n",
        "    pack = extractor.invoke(\n",
        "        [\n",
        "            SystemMessage(content=RESEARCH_SYSTEM),\n",
        "            HumanMessage(\n",
        "                content=(\n",
        "                    f\"As-of date: {state['as_of']}\\n\"\n",
        "                    f\"Recency days: {state['recency_days']}\\n\\n\"\n",
        "                    f\"Raw results:\\n{raw}\"\n",
        "                )\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    dedup = {}\n",
        "    for e in pack.evidence:\n",
        "        if e.url:\n",
        "            dedup[e.url] = e\n",
        "    evidence = list(dedup.values())\n",
        "\n",
        "    if state.get(\"mode\") == \"open_book\":\n",
        "        as_of = date.fromisoformat(state[\"as_of\"])\n",
        "        cutoff = as_of - timedelta(days=int(state[\"recency_days\"]))\n",
        "        evidence = [e for e in evidence if (d := _iso_to_date(e.published_at)) and d >= cutoff]\n",
        "\n",
        "    return {\"evidence\": evidence}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
        "Produce a highly actionable outline for a technical blog post.\n",
        "\n",
        "Requirements:\n",
        "- 5‚Äì9 tasks, each with goal + 3‚Äì6 bullets + target_words.\n",
        "- Tags are flexible; do not force a fixed taxonomy.\n",
        "\n",
        "Grounding:\n",
        "- closed_book: evergreen, no evidence dependence.\n",
        "- hybrid: use evidence for up-to-date examples; mark those tasks requires_research=True and requires_citations=True.\n",
        "- open_book: weekly/news roundup:\n",
        "  - Set blog_kind=\"news_roundup\"\n",
        "  - No tutorial content unless requested\n",
        "  - If evidence is weak, plan should explicitly reflect that (don't invent events).\n",
        "\n",
        "Output must match Plan schema.\n",
        "\"\"\"\n",
        "\n",
        "def orchestrator_node(state: State) -> dict:\n",
        "    planner = llm.with_structured_output(Plan)\n",
        "    mode = state.get(\"mode\", \"closed_book\")\n",
        "    evidence = state.get(\"evidence\", [])\n",
        "\n",
        "    forced_kind = \"news_roundup\" if mode == \"open_book\" else None\n",
        "\n",
        "    plan = planner.invoke(\n",
        "        [\n",
        "            SystemMessage(content=ORCH_SYSTEM),\n",
        "            HumanMessage(\n",
        "                content=(\n",
        "                    f\"Topic: {state['topic']}\\n\"\n",
        "                    f\"Mode: {mode}\\n\"\n",
        "                    f\"As-of: {state['as_of']} (recency_days={state['recency_days']})\\n\"\n",
        "                    f\"{'Force blog_kind=news_roundup' if forced_kind else ''}\\n\\n\"\n",
        "                    f\"Evidence:\\n{[e.model_dump() for e in evidence][:16]}\"\n",
        "                )\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    if forced_kind:\n",
        "        plan.blog_kind = \"news_roundup\"\n",
        "\n",
        "    return {\"plan\": plan}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Fanout Function\n",
        "\n",
        "This function distributes tasks to worker nodes. Each task is sent to a worker node in parallel using LangGraph's Send mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fanout(state: State):\n",
        "    assert state[\"plan\"] is not None\n",
        "    return [\n",
        "        Send(\n",
        "            \"worker\",\n",
        "            {\n",
        "                \"task\": task.model_dump(),\n",
        "                \"topic\": state[\"topic\"],\n",
        "                \"mode\": state[\"mode\"],\n",
        "                \"as_of\": state[\"as_of\"],\n",
        "                \"recency_days\": state[\"recency_days\"],\n",
        "                \"plan\": state[\"plan\"].model_dump(),\n",
        "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
        "            },\n",
        "        )\n",
        "        for task in state[\"plan\"].tasks\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Worker Node\n",
        "\n",
        "Each worker writes one section of the blog. Workers run in parallel, each handling one task. They follow the plan's bullets, stay within word count, and cite evidence when required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
        "Write ONE section of a technical blog post in Markdown.\n",
        "\n",
        "Constraints:\n",
        "- Cover ALL bullets in order.\n",
        "- Target words ¬±15%.\n",
        "- Output only section markdown starting with \"## <Section Title>\".\n",
        "\n",
        "Scope guard:\n",
        "- If blog_kind==\"news_roundup\", do NOT drift into tutorials (scraping/RSS/how to fetch).\n",
        "  Focus on events + implications.\n",
        "\n",
        "Grounding:\n",
        "- If mode==\"open_book\": do not introduce any specific event/company/model/funding/policy claim unless supported by provided Evidence URLs.\n",
        "  For each supported claim, attach a Markdown link ([Source](URL)).\n",
        "  If unsupported, write \"Not found in provided sources.\"\n",
        "- If requires_citations==true (hybrid tasks): cite Evidence URLs for external claims.\n",
        "\n",
        "Code:\n",
        "- If requires_code==true, include at least one minimal snippet.\n",
        "\"\"\"\n",
        "\n",
        "def worker_node(payload: dict) -> dict:\n",
        "    task = Task(**payload[\"task\"])\n",
        "    plan = Plan(**payload[\"plan\"])\n",
        "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
        "\n",
        "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
        "    evidence_text = \"\\n\".join(\n",
        "        f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\"\n",
        "        for e in evidence[:20]\n",
        "    )\n",
        "\n",
        "    section_md = llm.invoke(\n",
        "        [\n",
        "            SystemMessage(content=WORKER_SYSTEM),\n",
        "            HumanMessage(\n",
        "                content=(\n",
        "                    f\"Blog title: {plan.blog_title}\\n\"\n",
        "                    f\"Audience: {plan.audience}\\n\"\n",
        "                    f\"Tone: {plan.tone}\\n\"\n",
        "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
        "                    f\"Constraints: {plan.constraints}\\n\"\n",
        "                    f\"Topic: {payload['topic']}\\n\"\n",
        "                    f\"Mode: {payload.get('mode')}\\n\"\n",
        "                    f\"As-of: {payload.get('as_of')} (recency_days={payload.get('recency_days')})\\n\\n\"\n",
        "                    f\"Section title: {task.title}\\n\"\n",
        "                    f\"Goal: {task.goal}\\n\"\n",
        "                    f\"Target words: {task.target_words}\\n\"\n",
        "                    f\"Tags: {task.tags}\\n\"\n",
        "                    f\"requires_research: {task.requires_research}\\n\"\n",
        "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
        "                    f\"requires_code: {task.requires_code}\\n\"\n",
        "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
        "                    f\"Evidence (ONLY cite these URLs):\\n{evidence_text}\\n\"\n",
        "                )\n",
        "            ),\n",
        "        ]\n",
        "    ).content.strip()\n",
        "\n",
        "    return {\"sections\": [(task.id, section_md)]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Reducer Subgraph\n",
        "\n",
        "The reducer is a subgraph with three nodes:\n",
        "1. **merge_content**: Combines all worker sections into one document\n",
        "2. **decide_images**: Decides if images are needed and where to place them\n",
        "3. **generate_and_place_images**: Generates images using Gemini API and inserts them into the markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_content(state: State) -> dict:\n",
        "    plan = state[\"plan\"]\n",
        "    if plan is None:\n",
        "        raise ValueError(\"merge_content called without plan.\")\n",
        "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
        "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
        "    merged_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
        "    return {\"merged_md\": merged_md}\n",
        "\n",
        "\n",
        "DECIDE_IMAGES_SYSTEM = \"\"\"You are an expert technical editor.\n",
        "Decide if images/diagrams are needed for THIS blog.\n",
        "\n",
        "Rules:\n",
        "- Max 3 images total.\n",
        "- Each image must materially improve understanding (diagram/flow/table-like visual).\n",
        "- Insert placeholders exactly: [[IMAGE_1]], [[IMAGE_2]], [[IMAGE_3]].\n",
        "- If no images needed: md_with_placeholders must equal input and images=[].\n",
        "- Avoid decorative images; prefer technical diagrams with short labels.\n",
        "Return strictly GlobalImagePlan.\n",
        "\"\"\"\n",
        "\n",
        "def decide_images(state: State) -> dict:\n",
        "    planner = llm.with_structured_output(GlobalImagePlan)\n",
        "    merged_md = state[\"merged_md\"]\n",
        "    plan = state[\"plan\"]\n",
        "    assert plan is not None\n",
        "\n",
        "    image_plan = planner.invoke(\n",
        "        [\n",
        "            SystemMessage(content=DECIDE_IMAGES_SYSTEM),\n",
        "            HumanMessage(\n",
        "                content=(\n",
        "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
        "                    f\"Topic: {state['topic']}\\n\\n\"\n",
        "                    \"Insert placeholders + propose image prompts.\\n\\n\"\n",
        "                    f\"{merged_md}\"\n",
        "                )\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"md_with_placeholders\": image_plan.md_with_placeholders,\n",
        "        \"image_specs\": [img.model_dump() for img in image_plan.images],\n",
        "    }\n",
        "\n",
        "\n",
        "def _gemini_generate_image_bytes(prompt: str) -> bytes:\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"GOOGLE_API_KEY is not set.\")\n",
        "\n",
        "    client = genai.Client(api_key=api_key)\n",
        "\n",
        "    resp = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash-image\",\n",
        "        contents=prompt,\n",
        "        config=types.GenerateContentConfig(\n",
        "            response_modalities=[\"IMAGE\"],\n",
        "            safety_settings=[\n",
        "                types.SafetySetting(\n",
        "                    category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "                    threshold=\"BLOCK_ONLY_HIGH\",\n",
        "                )\n",
        "            ],\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    parts = getattr(resp, \"parts\", None)\n",
        "    if not parts and getattr(resp, \"candidates\", None):\n",
        "        try:\n",
        "            parts = resp.candidates[0].content.parts\n",
        "        except Exception:\n",
        "            parts = None\n",
        "\n",
        "    if not parts:\n",
        "        raise RuntimeError(\"No image content returned (safety/quota/SDK change).\")\n",
        "\n",
        "    for part in parts:\n",
        "        inline = getattr(part, \"inline_data\", None)\n",
        "        if inline and getattr(inline, \"data\", None):\n",
        "            return inline.data\n",
        "\n",
        "    raise RuntimeError(\"No inline image bytes found in response.\")\n",
        "\n",
        "\n",
        "def _safe_slug(title: str) -> str:\n",
        "    s = title.strip().lower()\n",
        "    s = re.sub(r\"[^a-z0-9 _-]+\", \"\", s)\n",
        "    s = re.sub(r\"\\s+\", \"_\", s).strip(\"_\")\n",
        "    return s or \"blog\"\n",
        "\n",
        "\n",
        "def generate_and_place_images(state: State) -> dict:\n",
        "    plan = state[\"plan\"]\n",
        "    assert plan is not None\n",
        "\n",
        "    md = state.get(\"md_with_placeholders\") or state[\"merged_md\"]\n",
        "    image_specs = state.get(\"image_specs\", []) or []\n",
        "\n",
        "    if not image_specs:\n",
        "        filename = f\"{_safe_slug(plan.blog_title)}.md\"\n",
        "        Path(filename).write_text(md, encoding=\"utf-8\")\n",
        "        return {\"final\": md}\n",
        "\n",
        "    images_dir = Path(\"images\")\n",
        "    images_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for spec in image_specs:\n",
        "        placeholder = spec[\"placeholder\"]\n",
        "        filename = spec[\"filename\"]\n",
        "        out_path = images_dir / filename\n",
        "\n",
        "        if not out_path.exists():\n",
        "            try:\n",
        "                img_bytes = _gemini_generate_image_bytes(spec[\"prompt\"])\n",
        "                out_path.write_bytes(img_bytes)\n",
        "            except Exception as e:\n",
        "                prompt_block = (\n",
        "                    f\"> **[IMAGE GENERATION FAILED]** {spec.get('caption','')}\\n>\\n\"\n",
        "                    f\"> **Alt:** {spec.get('alt','')}\\n>\\n\"\n",
        "                    f\"> **Prompt:** {spec.get('prompt','')}\\n>\\n\"\n",
        "                    f\"> **Error:** {e}\\n\"\n",
        "                )\n",
        "                md = md.replace(placeholder, prompt_block)\n",
        "                continue\n",
        "\n",
        "        img_md = f\"![{spec['alt']}](images/{filename})\\n*{spec['caption']}*\"\n",
        "        md = md.replace(placeholder, img_md)\n",
        "\n",
        "    filename = f\"{_safe_slug(plan.blog_title)}.md\"\n",
        "    Path(filename).write_text(md, encoding=\"utf-8\")\n",
        "    return {\"final\": md}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reducer_graph = StateGraph(State)\n",
        "reducer_graph.add_node(\"merge_content\", merge_content)\n",
        "reducer_graph.add_node(\"decide_images\", decide_images)\n",
        "reducer_graph.add_node(\"generate_and_place_images\", generate_and_place_images)\n",
        "reducer_graph.add_edge(START, \"merge_content\")\n",
        "reducer_graph.add_edge(\"merge_content\", \"decide_images\")\n",
        "reducer_graph.add_edge(\"decide_images\", \"generate_and_place_images\")\n",
        "reducer_graph.add_edge(\"generate_and_place_images\", END)\n",
        "reducer_subgraph = reducer_graph.compile()\n",
        "\n",
        "reducer_subgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Build Main Graph\n",
        "\n",
        "Now we build the main LangGraph workflow that connects all nodes:\n",
        "1. START ‚Üí router\n",
        "2. router ‚Üí research (if needed) OR router ‚Üí orchestrator\n",
        "3. research ‚Üí orchestrator\n",
        "4. orchestrator ‚Üí workers (fanout to multiple workers in parallel)\n",
        "5. workers ‚Üí reducer\n",
        "6. reducer ‚Üí END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "g = StateGraph(State)\n",
        "g.add_node(\"router\", router_node)\n",
        "g.add_node(\"research\", research_node)\n",
        "g.add_node(\"orchestrator\", orchestrator_node)\n",
        "g.add_node(\"worker\", worker_node)\n",
        "g.add_node(\"reducer\", reducer_subgraph)\n",
        "\n",
        "g.add_edge(START, \"router\")\n",
        "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
        "g.add_edge(\"research\", \"orchestrator\")\n",
        "\n",
        "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
        "g.add_edge(\"worker\", \"reducer\")\n",
        "g.add_edge(\"reducer\", END)\n",
        "\n",
        "app = g.compile()\n",
        "app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run(topic: str, as_of: Optional[str] = None):\n",
        "    if as_of is None:\n",
        "        as_of = date.today().isoformat()\n",
        "\n",
        "    out = app.invoke(\n",
        "        {\n",
        "            \"topic\": topic,\n",
        "            \"mode\": \"\",\n",
        "            \"needs_research\": False,\n",
        "            \"queries\": [],\n",
        "            \"evidence\": [],\n",
        "            \"plan\": None,\n",
        "            \"as_of\": as_of,\n",
        "            \"recency_days\": 7,\n",
        "            \"sections\": [],\n",
        "            \"merged_md\": \"\",\n",
        "            \"md_with_placeholders\": \"\",\n",
        "            \"image_specs\": [],\n",
        "            \"final\": \"\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    plan: Plan = out[\"plan\"]\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\"TOPIC:\", topic)\n",
        "    print(\"AS_OF:\", out.get(\"as_of\"), \"RECENCY_DAYS:\", out.get(\"recency_days\"))\n",
        "    print(\"MODE:\", out.get(\"mode\"))\n",
        "    print(\"BLOG_KIND:\", plan.blog_kind)\n",
        "    print(\"NEEDS_RESEARCH:\", out.get(\"needs_research\"))\n",
        "    print(\"QUERIES:\", (out.get(\"queries\") or [])[:6])\n",
        "    print(\"EVIDENCE_COUNT:\", len(out.get(\"evidence\", [])))\n",
        "    if out.get(\"evidence\"):\n",
        "        print(\"EVIDENCE_SAMPLE:\", [e.model_dump() for e in out[\"evidence\"][:2]])\n",
        "    print(\"TASKS:\", len(plan.tasks))\n",
        "    print(\"SAVED_MD_CHARS:\", len(out.get(\"final\", \"\")))\n",
        "    print(\"=\" * 100 + \"\\n\")\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Example Execution\n",
        "\n",
        "Let's run the agent with an example topic. This will:\n",
        "1. Route the topic to determine if research is needed\n",
        "2. Perform research if needed\n",
        "3. Create a detailed blog plan\n",
        "4. Write all sections in parallel\n",
        "5. Merge sections and optionally generate images\n",
        "6. Save the final blog as a markdown file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = run(\"Write a blog on Self Attention\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 15: View the Blog Plan\n",
        "\n",
        "Let's examine the plan that was created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if result and result.get(\"plan\"):\n",
        "    plan = result[\"plan\"]\n",
        "    print(f\"Blog Title: {plan.blog_title}\")\n",
        "    print(f\"Audience: {plan.audience}\")\n",
        "    print(f\"Tone: {plan.tone}\")\n",
        "    print(f\"Blog Kind: {plan.blog_kind}\")\n",
        "    print(f\"\\nNumber of Sections: {len(plan.tasks)}\\n\")\n",
        "    for task in plan.tasks:\n",
        "        print(f\"Section {task.id}: {task.title}\")\n",
        "        print(f\"  Goal: {task.goal}\")\n",
        "        print(f\"  Target Words: {task.target_words}\")\n",
        "        print(f\"  Requires Research: {task.requires_research}\")\n",
        "        print(f\"  Requires Citations: {task.requires_citations}\")\n",
        "        print(f\"  Requires Code: {task.requires_code}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: View the Generated Blog\n",
        "\n",
        "Let's display the final blog content:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 16: Verify Output Files\n",
        "\n",
        "Check that the blog markdown file was created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "if result and result.get(\"plan\"):\n",
        "    plan = result[\"plan\"]\n",
        "    filename = f\"{_safe_slug(plan.blog_title)}.md\"\n",
        "    filepath = Path(filename)\n",
        "    \n",
        "    if filepath.exists():\n",
        "        print(f\"‚úÖ Blog saved to: {filepath.absolute()}\")\n",
        "        print(f\"File size: {filepath.stat().st_size} bytes\")\n",
        "    else:\n",
        "        print(f\"‚ùå File not found: {filename}\")\n",
        "    \n",
        "    images_dir = Path(\"images\")\n",
        "    if images_dir.exists():\n",
        "        image_files = list(images_dir.glob(\"*\"))\n",
        "        if image_files:\n",
        "            print(f\"\\n‚úÖ Images directory contains {len(image_files)} file(s):\")\n",
        "            for img in image_files:\n",
        "                print(f\"  - {img.name}\")\n",
        "        else:\n",
        "            print(\"\\nüìù No images were generated for this blog\")\n",
        "    else:\n",
        "        print(\"\\nüìù No images directory (no images were generated)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(result[\"final\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Input\n",
        "- **Topic**: A string describing what blog to write (e.g., \"Write a blog on Self Attention\", \"State of Multimodal LLMs in 2026\")\n",
        "- **Optional**: `as_of` date for time-sensitive topics\n",
        "\n",
        "### Output\n",
        "- **Markdown file**: A complete blog post saved as `{blog_title}.md` in the current directory\n",
        "- **Images directory**: If images are generated, they are saved in `images/` folder\n",
        "- **State object**: Contains the full execution state including:\n",
        "  - The blog plan with all tasks\n",
        "  - Evidence collected from web research (if any)\n",
        "  - Image specifications\n",
        "  - Final markdown content\n",
        "\n",
        "### Workflow\n",
        "1. **Router** analyzes the topic and decides if research is needed\n",
        "2. **Research** (if needed) searches the web using Tavily API\n",
        "3. **Orchestrator** creates a detailed plan with 5-9 sections\n",
        "4. **Workers** write each section in parallel\n",
        "5. **Reducer** merges sections, decides on images, generates images, and saves the final blog\n",
        "\n",
        "The agent automatically handles:\n",
        "- Citation management for research-backed claims\n",
        "- Parallel section writing for efficiency\n",
        "- Image generation when diagrams would help understanding\n",
        "- Proper markdown formatting\n",
        "- File saving with appropriate naming"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
